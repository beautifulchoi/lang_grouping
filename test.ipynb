{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# tensor1 = torch.tensor([1, 3, 5, 7, 9])\n",
    "# tensor2 = torch.tensor([1, 2, 3, 6, 5])\n",
    "\n",
    "# result = np.intersect1d(tensor1, tensor2)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def find_overlap_cls(obj1: torch.Tensor, obj2: torch.Tensor):\n",
    "    \"\"\"\n",
    "    find overlapped class between two masks\n",
    "    \n",
    "    param\n",
    "     - obj1,2: GT class segmentation map\n",
    "     \n",
    "    return: overlapped class mask map\n",
    "    \"\"\"\n",
    "\n",
    "    cls1 = torch.unique(obj1)\n",
    "    cls2 = torch.unique(obj2)\n",
    "\n",
    "    overlapped_cls = torch.from_numpy(np.intersect1d(cls1, cls2))\n",
    "    \n",
    "    #ovl cls 만 열고 나머지는 -1으로 만들어\n",
    "    obj_mask1 = torch.where(torch.isin(obj1, overlapped_cls), obj1, -1.)\n",
    "    obj_mask2 = torch.where(torch.isin(obj2, overlapped_cls), obj2, -1.)\n",
    "\n",
    "\n",
    "    return obj_mask1, obj_mask2, overlapped_cls\n",
    "\n",
    "obj1=np.array(Image.open(\"/home/gaussian-grouping/data/lerf_mask/teatime/object_mask/frame_00001.png\").convert(\"L\"))\n",
    "obj2= np.array(Image.open(\"/home/gaussian-grouping/data/lerf_mask/teatime/object_mask/frame_00100.png\").convert(\"L\"))\n",
    "\n",
    "\n",
    "obj1=torch.from_numpy(obj1)\n",
    "obj2=torch.from_numpy(obj2)\n",
    "\n",
    "m1,m2, ov=find_overlap_cls(obj1, obj2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1.,   0.,  25.,  48.,  52.,  59.,  62.,  63.,  73.,  75.,  81.,  97.,\n",
       "         99., 103., 110., 118.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(features, instance_labels, temperature):\n",
    "    \"\"\"\n",
    "    https://github.com/yashbhalgat/Contrastive-Lift/blob/main/model/loss/loss.py\n",
    "    compute the language feature's contrastive loss \n",
    "    param :\n",
    "        - f_map : rendered semantic featrue of language \n",
    "        - obj_map : target's object instance map\n",
    "\n",
    "    return :\n",
    "        computed loss value\n",
    "    \"\"\"\n",
    "\n",
    "    bsize = features.size(0)\n",
    "    masks = instance_labels.view(-1, 1).repeat(1, bsize).eq_(instance_labels.clone())\n",
    "    masks = masks.fill_diagonal_(0, wrap=False)\n",
    "\n",
    "    # compute similarity matrix based on Euclidean distance\n",
    "    distance_sq = torch.pow(features.unsqueeze(1) - features.unsqueeze(0), 2).sum(dim=-1)\n",
    "    # temperature = 1 for positive pairs and temperature for negative pairs\n",
    "    temperature = torch.ones_like(distance_sq) * temperature\n",
    "    temperature = torch.where(masks==1, temperature, torch.ones_like(temperature))\n",
    "\n",
    "    similarity_kernel = torch.exp(-distance_sq/temperature)\n",
    "    logits = torch.exp(similarity_kernel)\n",
    "\n",
    "    p = torch.mul(logits, masks).sum(dim=-1)\n",
    "    Z = logits.sum(dim=-1)\n",
    "\n",
    "    prob = torch.div(p, Z)\n",
    "    prob_masked = torch.masked_select(prob, prob.ne(0))\n",
    "    loss = -prob_masked.log().sum()/bsize\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def _sample_indices(num_samples, size):\n",
    "    indices = random.sample(range(size), min(num_samples, size))\n",
    "    return torch.tensor(indices)\n",
    "\n",
    "def _rbf_kernel(x1, x2, gamma=1.0):\n",
    "    diff = x1.unsqueeze(1) - x2.unsqueeze(0)\n",
    "    squared_diff = torch.sum(diff**2, dim=-1)\n",
    "    return torch.exp(-gamma * squared_diff)\n",
    "\n",
    "def contrastive_loss(features, objects, margin=1.0, gamma=1.0, num_samples=1000):\n",
    "    loss = 0.0\n",
    "    for class_id in torch.unique(objects):\n",
    "        class_objects = (objects == class_id).float()\n",
    "        num_objects = torch.sum(class_objects)\n",
    "        \n",
    "        class_features = features * class_objects.unsqueeze(-1)\n",
    "        \n",
    "        if num_objects <= 1:\n",
    "            continue\n",
    "        \n",
    "        #sample_indices = _sample_indices(num_samples, num_objects)\n",
    "        sampled_indices = torch.randint(0, features.shape[0] * features.shape[1], (num_samples,))\n",
    "        class_features = class_features.view(-1, class_features.shape[-1]) # (H*W , 3)\n",
    "        sampled_class_features = class_features[sampled_indices]\n",
    "        print(sampled_class_features, sampled_indices)\n",
    "        if num_samples <= 100:\n",
    "            ValueError(\"num_sample is too low\")\n",
    "        \n",
    "        positive_similarities = _rbf_kernel(sampled_class_features, sampled_class_features, gamma)\n",
    "        positive_similarities = positive_similarities.triu(diagonal=1)\n",
    "        num_positive_pairs = torch.sum(positive_similarities > 0)\n",
    "        \n",
    "        negative_similarities = _rbf_kernel(sampled_class_features, features, gamma)\n",
    "        negative_similarities = negative_similarities * (1 - class_objects.unsqueeze(-1))\n",
    "        \n",
    "        loss += torch.sum(torch.relu(margin - positive_similarities)) / num_positive_pairs\n",
    "        loss += torch.sum(torch.relu(negative_similarities - margin)) / (num_samples * (features.size(0) - num_samples))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.0\n",
    "objects = m1\n",
    "features = lang_f.clone()\n",
    "num_samples =4096\n",
    "gamma = 0.01\n",
    "for class_id in torch.unique(objects): # obj id 0인경우 > class_objects에서 마스킹 됨\n",
    "    class_objects = (objects == class_id).float() # 해당 클래스에 대한 픽셀들\n",
    "    num_obj_pixel = torch.sum(class_objects)\n",
    "    \n",
    "    class_features = features * class_objects.unsqueeze(-1)\n",
    "    class_features = torch.where(class_features>=0, class_features, 0) \n",
    "    if num_obj_pixel <= 1:\n",
    "        continue\n",
    "    \n",
    "    #sample_indices = _sample_indices(num_samples, num_objects)\n",
    "    sampled_indices = torch.randint(0, features.shape[0] * features.shape[1], (num_samples,)) #u개의 픽셀을 \"전체에서\" 뽑음 \n",
    "    \n",
    "    class_features = class_features.view(-1, class_features.shape[-1]) # (H*W , 3)\n",
    "    sampled_cls_indices = sampled_indices[class_objects.view(-1,1)[sampled_indices].squeeze(-1) == 1.] #샘플링한 픽셀 중에서 클래스에 해당하는 픽셀들 \n",
    "    sampled_class_features = class_features[sampled_cls_indices] # pos pair \n",
    "\n",
    "    \n",
    "    positive_similarities = _rbf_kernel(sampled_class_features, sampled_class_features, gamma) # 샘플링을 같은 클래스에서 해야함\n",
    "    \n",
    "    positive_similarities = positive_similarities.triu(diagonal=1)\n",
    "    num_positive_pairs = torch.sum(positive_similarities > 0)\n",
    "    \n",
    "    features_flatten = features.view(-1, features.shape[-1])\n",
    "    sampled_features = features_flatten[sampled_indices]\n",
    "    \n",
    "    negative_similarities = _rbf_kernel(sampled_features, sampled_features, gamma)\n",
    "    negative_similarities = negative_similarities.triu(diagonal=1)\n",
    "    num_total_pairs = torch.sum(negative_similarities > 0)\n",
    "\n",
    "    #negative_similarities = negative_similarities * (1 - class_objects.unsqueeze(-1))\n",
    "    \n",
    "    # positive_similarities= torch.exp(positive_similarities).sum()\n",
    "    # negative_similarities= torch.exp(negative_similarities).sum()\n",
    "\n",
    "    # logits= -torch.log(positive_similarities/negative_similarities) / num_total_pairs\n",
    "    \n",
    "    # # Positive pairs loss\n",
    "    # if num_positive_pairs > 0:\n",
    "    #     positive_loss = -torch.log(positive_similarities.sum() / num_positive_pairs)\n",
    "    #     loss += positive_loss\n",
    "    \n",
    "    # # Negative pairs loss\n",
    "    # if (1 - class_objects).sum() > 0:\n",
    "    #     negative_loss = -torch.log(negative_similarities.sum() / (1 - class_objects).sum())\n",
    "    #     loss += negative_loss\n",
    "    \n",
    "#loss /= len(torch.unique(objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [730, 988] at index 0 does not match the shape of the indexed tensor [721240, 3] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m sampled_neg_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], (num_samples,))\n\u001b[1;32m     32\u001b[0m sampled_neg_features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])[sampled_neg_indices]\n\u001b[0;32m---> 33\u001b[0m sampled_neg_class_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43m[\u001b[49m\u001b[43mclass_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneg_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mneg_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnegative_class_ids\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     34\u001b[0m negative_similarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([_rbf_kernel(sampled_class_features, sampled_neg_features, gamma) \u001b[38;5;28;01mfor\u001b[39;00m sampled_class_features \u001b[38;5;129;01min\u001b[39;00m sampled_neg_class_features])\n\u001b[1;32m     35\u001b[0m num_negative_pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(negative_similarities \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[99], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m sampled_neg_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], (num_samples,))\n\u001b[1;32m     32\u001b[0m sampled_neg_features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])[sampled_neg_indices]\n\u001b[0;32m---> 33\u001b[0m sampled_neg_class_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mclass_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneg_id\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neg_id \u001b[38;5;129;01min\u001b[39;00m negative_class_ids])\n\u001b[1;32m     34\u001b[0m negative_similarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([_rbf_kernel(sampled_class_features, sampled_neg_features, gamma) \u001b[38;5;28;01mfor\u001b[39;00m sampled_class_features \u001b[38;5;129;01min\u001b[39;00m sampled_neg_class_features])\n\u001b[1;32m     35\u001b[0m num_negative_pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(negative_similarities \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [730, 988] at index 0 does not match the shape of the indexed tensor [721240, 3] at index 0"
     ]
    }
   ],
   "source": [
    "loss = 0.0\n",
    "objects = m1\n",
    "features = lang_f.clone()\n",
    "num_samples = 4096\n",
    "gamma = 0.01\n",
    "\n",
    "for class_id in torch.unique(objects):\n",
    "    class_objects = (objects == class_id).float()\n",
    "    num_obj_pixel = torch.sum(class_objects)\n",
    "    \n",
    "    if num_obj_pixel <= 1:\n",
    "        continue\n",
    "    \n",
    "    class_features = features * class_objects.unsqueeze(-1)\n",
    "    class_features = torch.where(class_features >= 0, class_features, 0) \n",
    "\n",
    "    # Sample indices from all pixels\n",
    "    sampled_indices = torch.randint(0, features.shape[0] * features.shape[1], (num_samples,))\n",
    "    \n",
    "    class_features = class_features.view(-1, class_features.shape[-1])\n",
    "    # Filter out sampled indices that belong to the current class\n",
    "    sampled_cls_indices = sampled_indices[class_objects.view(-1,1)[sampled_indices].squeeze(-1) == 1.] \n",
    "    sampled_class_features = class_features[sampled_cls_indices]\n",
    "\n",
    "    # Compute positive similarities\n",
    "    positive_similarities = _rbf_kernel(sampled_class_features, sampled_class_features, gamma) \n",
    "    num_positive_pairs = torch.sum(positive_similarities > 0)\n",
    "    \n",
    "    # Compute negative similarities between features of different classes\n",
    "    negative_class_ids = torch.unique(objects[objects != class_id])\n",
    "    sampled_neg_indices = torch.randint(0, features.shape[0] * features.shape[1], (num_samples,))\n",
    "    sampled_neg_features = features.view(-1, features.shape[-1])[sampled_neg_indices]\n",
    "    sampled_neg_class_features = torch.stack([class_features[objects == neg_id] for neg_id in negative_class_ids])\n",
    "    negative_similarities = torch.cat([_rbf_kernel(sampled_class_features, sampled_neg_features, gamma) for sampled_class_features in sampled_neg_class_features])\n",
    "    num_negative_pairs = torch.sum(negative_similarities > 0)\n",
    "    \n",
    "    # Compute logits for contrastive loss\n",
    "    logits = -torch.log(num_positive_pairs / num_negative_pairs)\n",
    "    \n",
    "    loss += logits\n",
    "\n",
    "# Normalize the loss by the number of classes\n",
    "loss /= len(torch.unique(objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(class_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "lang_f = torch.rand(730,988,3)\n",
    "m1,m2, ovl_cls=find_overlap_cls(obj1, obj2)\n",
    "# m1, m2는 마스크 , ovl cls는 인덱스 리스트\n",
    "#m1의 각각의 인스탄스 영역들에 대해서, -1 값인 것은 제외시키고 나머지 클래스들에 대해서 분리(마스크) \n",
    "# 각 클래스의 인스탄스 영역에 대해서 \n",
    "\n",
    "#contrastive_loss(lang_f, m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaussian_grouping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
